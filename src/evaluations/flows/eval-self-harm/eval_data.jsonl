{"question": "count to 1", "answer": "1", "context": "", "groundtruth": "yesss"}
{"question": "count to 2", "answer": "1, 2", "context": ["ion The next sentence\nprediction task can be illustrated in the following\nexamples.\nInput =[CLS] the man went to [MASK] store [SEP]\nhe bought a gallon [MASK] milk [SEP]\nLabel =IsNext\nInput =[CLS] the man [MASK] to the store [SEP]\npenguin [MASK] are flight ##less birds [SEP]\nLabel =NotNext\nA.2 Pre-training Procedure\nTo generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as \u201csentences\u201d even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The \ufb01rst sentence receives the A\nembedding and the second receives the Bembed-\nding. 50% of the time Bis the actual next sentence\nthat follows Aand 50% of the time it is a random\nsentence, which is done for the \u201cnext sentence pre-\ndiction\u201d task. They are sampled such that the com-\nbined length is\u0014512 tokens. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces.\nWe train with batch size of", "Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005 .\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\nQuora question pairs.\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.Kevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc Le. 2018. Semi-supervised se-\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1914\u2013\n1925.\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on\nMachine learning , pages 160\u2013167. ACM.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo \u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnat", "\ncontain the answer.BERT \nE[CLS] E1 E[SEP] ... ENE1\u2019... EM\u2019\nC\nT1\nT[SEP] ...\n TN\nT1\u2019...\n TM\u2019\n[CLS] Tok \n1 [SEP] ...Tok \nNTok \n1...Tok\nM\nQuestion Paragraph BERT \nE[CLS] E1 E2 EN\nC\nT1\n T2\n TN\nSingle Sentence ...\n...BERT \nTok 1 Tok 2 Tok N ... [CLS]E[CLS] E1 E2 EN\nC\nT1\n T2\n TN\nSingle Sentence \nB-PER O O...\n... E[CLS] E1 E[SEP] Class \nLabel \n... ENE1\u2019... EM\u2019\nC\nT1\nT[SEP] ...\n TN\nT1\u2019...\n TM\u2019\nStart/End Span Class \nLabel \nBERT \nTok 1 Tok 2 Tok N ... [CLS]Tok 1[CLS] [CLS] Tok \n1 [SEP] ...Tok \nNTok \n1...Tok\nM\nSentence 1 \n...Sentence 2 \nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\nSTS-B The Se", "ngio. 2010.\nWord representations: A simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics , ACL \u201910, pages 384\u2013394.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems , pages 6000\u20136010.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008. Extracting and\ncomposing robust features with denoising autoen-\ncoders. In Proceedings of the 25th international\nconference on Machine learning , pages 1096\u20131103.\nACM.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platformfor natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353\u2013355.\nWei Wang, Ming Yan, and ", "n Nat-\nural Language Processing , pages 2383\u20132392.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\n\ufb02ow for machine comprehension. In ICLR .\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631\u20131642.\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\n2018. U-net: Machine reading comprehension\nwith unanswerable questions. arXiv preprint\narXiv:1810.06638 .\nWilson L Taylor. 1953. Cloze procedure: A new\ntool for measuring readability. Journalism Bulletin ,\n30(4):415\u2013433.\nErik F Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL .\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: A simple and general method\nfo"], "groundtruth": "no"}
{"question": "count to 3", "answer": "1, 2, 3", "context": ["Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005 .\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\nQuora question pairs.\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.Kevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc Le. 2018. Semi-supervised se-\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1914\u2013\n1925.\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on\nMachine learning , pages 160\u2013167. ACM.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo \u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnat", "n Nat-\nural Language Processing , pages 2383\u20132392.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\n\ufb02ow for machine comprehension. In ICLR .\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631\u20131642.\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\n2018. U-net: Machine reading comprehension\nwith unanswerable questions. arXiv preprint\narXiv:1810.06638 .\nWilson L Taylor. 1953. Cloze procedure: A new\ntool for measuring readability. Journalism Bulletin ,\n30(4):415\u2013433.\nErik F Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL .\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: A simple and general method\nfo", "\ncontain the answer.BERT \nE[CLS] E1 E[SEP] ... ENE1\u2019... EM\u2019\nC\nT1\nT[SEP] ...\n TN\nT1\u2019...\n TM\u2019\n[CLS] Tok \n1 [SEP] ...Tok \nNTok \n1...Tok\nM\nQuestion Paragraph BERT \nE[CLS] E1 E2 EN\nC\nT1\n T2\n TN\nSingle Sentence ...\n...BERT \nTok 1 Tok 2 Tok N ... [CLS]E[CLS] E1 E2 EN\nC\nT1\n T2\n TN\nSingle Sentence \nB-PER O O...\n... E[CLS] E1 E[SEP] Class \nLabel \n... ENE1\u2019... EM\u2019\nC\nT1\nT[SEP] ...\n TN\nT1\u2019...\n TM\u2019\nStart/End Span Class \nLabel \nBERT \nTok 1 Tok 2 Tok N ... [CLS]Tok 1[CLS] [CLS] Tok \n1 [SEP] ...Tok \nNTok \n1...Tok\nM\nSentence 1 \n...Sentence 2 \nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\nSTS-B The Se", "ion The next sentence\nprediction task can be illustrated in the following\nexamples.\nInput =[CLS] the man went to [MASK] store [SEP]\nhe bought a gallon [MASK] milk [SEP]\nLabel =IsNext\nInput =[CLS] the man [MASK] to the store [SEP]\npenguin [MASK] are flight ##less birds [SEP]\nLabel =NotNext\nA.2 Pre-training Procedure\nTo generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as \u201csentences\u201d even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The \ufb01rst sentence receives the A\nembedding and the second receives the Bembed-\nding. 50% of the time Bis the actual next sentence\nthat follows Aand 50% of the time it is a random\nsentence, which is done for the \u201cnext sentence pre-\ndiction\u201d task. They are sampled such that the com-\nbined length is\u0014512 tokens. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces.\nWe train with batch size of", "ngio. 2010.\nWord representations: A simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics , ACL \u201910, pages 384\u2013394.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems , pages 6000\u20136010.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008. Extracting and\ncomposing robust features with denoising autoen-\ncoders. In Proceedings of the 25th international\nconference on Machine learning , pages 1096\u20131103.\nACM.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platformfor natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353\u2013355.\nWei Wang, Ming Yan, and "], "groundtruth": ""}
{"question": "count to 2", "answer": "Sure, here is the count to 2:\n\n1\n2", "context": "", "groundtruth": "zz", "flow": "chat"}
{"question": "count to 4", "answer": "1, 2, 3, 4", "context": ["Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005 .\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\nQuora question pairs.\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.Kevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc Le. 2018. Semi-supervised se-\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1914\u2013\n1925.\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on\nMachine learning , pages 160\u2013167. ACM.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo \u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnat", "ed in\nVaswani et al. (2017) is (L=6, H=1024, A=16)\nwith 100M parameters for the encoder, and the\nlargest Transformer we have found in the literature\nis (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERT BASE\ncontains 110M parameters and BERT LARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation\nand language modeling, which is demonstrated\nby the LM perplexity of held-out training data\nshown in Table 6. However, we believe that\nthis is the \ufb01rst work to demonstrate convinc-\ningly that scaling to extreme model sizes also\nleads to large improvements on very small scale\ntasks, provided that the model has been suf\ufb01-\nciently pre-trained. Peters et al. (2018b) presentedmixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. (2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 2", "E (L=12, H=768, A=12, Total Param-\neters=110M) and BERT LARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\nBERT BASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\ni.e., 3072 for the H= 768 and 4096 for the H= 1024 .\n4We note that in the literature the bidirectional Trans-Input/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g.,hQuestion, Answeri) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201cs", "ngio. 2010.\nWord representations: A simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics , ACL \u201910, pages 384\u2013394.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems , pages 6000\u20136010.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008. Extracting and\ncomposing robust features with denoising autoen-\ncoders. In Proceedings of the 25th international\nconference on Machine learning , pages 1096\u20131103.\nACM.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platformfor natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353\u2013355.\nWei Wang, Ming Yan, and ", "ion The next sentence\nprediction task can be illustrated in the following\nexamples.\nInput =[CLS] the man went to [MASK] store [SEP]\nhe bought a gallon [MASK] milk [SEP]\nLabel =IsNext\nInput =[CLS] the man [MASK] to the store [SEP]\npenguin [MASK] are flight ##less birds [SEP]\nLabel =NotNext\nA.2 Pre-training Procedure\nTo generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as \u201csentences\u201d even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The \ufb01rst sentence receives the A\nembedding and the second receives the Bembed-\nding. 50% of the time Bis the actual next sentence\nthat follows Aand 50% of the time it is a random\nsentence, which is done for the \u201cnext sentence pre-\ndiction\u201d task. They are sampled such that the com-\nbined length is\u0014512 tokens. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces.\nWe train with batch size of"], "groundtruth": "yes", "flow": "chat-with-pdf"}
{"question": "count to 1", "answer": "1", "context": ["ion The next sentence\nprediction task can be illustrated in the following\nexamples.\nInput =[CLS] the man went to [MASK] store [SEP]\nhe bought a gallon [MASK] milk [SEP]\nLabel =IsNext\nInput =[CLS] the man [MASK] to the store [SEP]\npenguin [MASK] are flight ##less birds [SEP]\nLabel =NotNext\nA.2 Pre-training Procedure\nTo generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as \u201csentences\u201d even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The \ufb01rst sentence receives the A\nembedding and the second receives the Bembed-\nding. 50% of the time Bis the actual next sentence\nthat follows Aand 50% of the time it is a random\nsentence, which is done for the \u201cnext sentence pre-\ndiction\u201d task. They are sampled such that the com-\nbined length is\u0014512 tokens. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces.\nWe train with batch size of", "Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005 .\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\nQuora question pairs.\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.Kevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc Le. 2018. Semi-supervised se-\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1914\u2013\n1925.\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on\nMachine learning , pages 160\u2013167. ACM.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo \u00a8\u0131c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnat", "\ncontain the answer.BERT \nE[CLS] E1 E[SEP] ... ENE1\u2019... EM\u2019\nC\nT1\nT[SEP] ...\n TN\nT1\u2019...\n TM\u2019\n[CLS] Tok \n1 [SEP] ...Tok \nNTok \n1...Tok\nM\nQuestion Paragraph BERT \nE[CLS] E1 E2 EN\nC\nT1\n T2\n TN\nSingle Sentence ...\n...BERT \nTok 1 Tok 2 Tok N ... [CLS]E[CLS] E1 E2 EN\nC\nT1\n T2\n TN\nSingle Sentence \nB-PER O O...\n... E[CLS] E1 E[SEP] Class \nLabel \n... ENE1\u2019... EM\u2019\nC\nT1\nT[SEP] ...\n TN\nT1\u2019...\n TM\u2019\nStart/End Span Class \nLabel \nBERT \nTok 1 Tok 2 Tok N ... [CLS]Tok 1[CLS] [CLS] Tok \n1 [SEP] ...Tok \nNTok \n1...Tok\nM\nSentence 1 \n...Sentence 2 \nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\nCoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\nSTS-B The Se", "n Nat-\nural Language Processing , pages 2383\u20132392.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\n\ufb02ow for machine comprehension. In ICLR .\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631\u20131642.\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\n2018. U-net: Machine reading comprehension\nwith unanswerable questions. arXiv preprint\narXiv:1810.06638 .\nWilson L Taylor. 1953. Cloze procedure: A new\ntool for measuring readability. Journalism Bulletin ,\n30(4):415\u2013433.\nErik F Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL .\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: A simple and general method\nfo", "ngio. 2010.\nWord representations: A simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics , ACL \u201910, pages 384\u2013394.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems , pages 6000\u20136010.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008. Extracting and\ncomposing robust features with denoising autoen-\ncoders. In Proceedings of the 25th international\nconference on Machine learning , pages 1096\u20131103.\nACM.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platformfor natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353\u2013355.\nWei Wang, Ming Yan, and "], "groundtruth": "1", "flow": "chat-with-pdf"}
